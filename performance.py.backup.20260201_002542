"""
Performance Optimizations for Employee Suite
Lightning-fast caching and response optimization
"""

from functools import wraps
from datetime import datetime, timedelta
from collections import OrderedDict
import hashlib
import json
import logging
import sys

logger = logging.getLogger(__name__)

# In-memory cache with LRU eviction (prevents memory exhaustion)
# Using OrderedDict for O(1) LRU operations
_cache = OrderedDict()
_cache_timestamps = {}
_cache_access_times = {}  # Track last access for LRU

# Cache limits to prevent memory exhaustion (critical for worker stability)
MAX_CACHE_ENTRIES = 100  # Maximum number of cache entries per worker
MAX_CACHE_SIZE_MB = 50  # Maximum cache size in MB (approximate)

# Cache TTLs (in seconds) - Balanced for performance and memory
CACHE_TTL_INVENTORY = 180  # 3 minutes for inventory (reduced from 5min to prevent memory issues)
CACHE_TTL_ORDERS = 120  # 2 minutes for orders (reduced from 3min)
CACHE_TTL_REPORTS = 300  # 5 minutes for reports (reduced from 10min to prevent crashes)
CACHE_TTL_STATS = 180  # 3 minutes for dashboard stats

def get_cache_key(prefix, *args, **kwargs):
    """Generate cache key from function arguments"""
    key_data = {
        'args': args,
        'kwargs': sorted(kwargs.items()) if kwargs else {}
    }
    key_string = json.dumps(key_data, sort_keys=True, default=str)
    key_hash = hashlib.md5(key_string.encode()).hexdigest()
    return f"{prefix}:{key_hash}"

def _get_cache_size_mb():
    """Estimate cache size in MB"""
    try:
        total_size = 0
        for key, value in _cache.items():
            # Rough estimate: key + value size
            try:
                key_size = sys.getsizeof(str(key))
                value_size = sys.getsizeof(str(value))
                total_size += key_size + value_size
            except Exception as e:
                logger.warning(f"Failed to calculate cache size for key {key}: {e}")
        return total_size / (1024 * 1024)  # Convert to MB
    except Exception as e:
        logger.warning(f"Failed to get cache size: {e}")
        return 0

def _evict_lru():
    """Evict least recently used cache entries"""
    try:
        # Remove oldest entries until under limit
        while len(_cache) >= MAX_CACHE_ENTRIES:
            # Remove oldest (first in OrderedDict)
            if _cache:
                oldest_key = next(iter(_cache))
                _cache.pop(oldest_key, None)
                _cache_timestamps.pop(oldest_key, None)
                _cache_access_times.pop(oldest_key, None)

        # Also check size limit
        cache_size_mb = _get_cache_size_mb()
        if cache_size_mb > MAX_CACHE_SIZE_MB:
            # Remove oldest entries until under size limit
            while cache_size_mb > MAX_CACHE_SIZE_MB and _cache:
                oldest_key = next(iter(_cache))
                _cache.pop(oldest_key, None)
                _cache_timestamps.pop(oldest_key, None)
                _cache_access_times.pop(oldest_key, None)
                cache_size_mb = _get_cache_size_mb()
    except Exception as e:
        logger.warning(f"Error during cache eviction: {e}")
        # If eviction fails, clear cache to prevent memory issues
        _cache.clear()
        _cache_timestamps.clear()
        _cache_access_times.clear()

def cache_result(ttl=CACHE_TTL_INVENTORY):
    """Decorator to cache function results with LRU eviction"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                # Generate cache key
                cache_key = get_cache_key(func.__name__, *args, **kwargs)

                # Check cache
                if cache_key in _cache:
                    timestamp = _cache_timestamps.get(cache_key)
                    if timestamp and (datetime.utcnow() - timestamp).total_seconds() < ttl:
                        # Cache hit - move to end (most recently used)
                        _cache.move_to_end(cache_key)
                        _cache_access_times[cache_key] = datetime.utcnow()
                        logger.debug(f"Cache HIT: {func.__name__}")
                        return _cache[cache_key]
                    else:
                        # Expired, remove it
                        _cache.pop(cache_key, None)
                        _cache_timestamps.pop(cache_key, None)
                        _cache_access_times.pop(cache_key, None)

                # Cache miss - execute function
                logger.debug(f"Cache MISS: {func.__name__}")
                result = func(*args, **kwargs)

                # Evict if needed before storing
                _evict_lru()

                # Store in cache (at end = most recently used)
                _cache[cache_key] = result
                _cache_timestamps[cache_key] = datetime.utcnow()
                _cache_access_times[cache_key] = datetime.utcnow()

                # Move to end to mark as recently used
                _cache.move_to_end(cache_key)

                return result
            except MemoryError:
                # Memory error - clear cache and retry without caching
                logger.error("Memory error in cache - clearing cache")
                _cache.clear()
                _cache_timestamps.clear()
                _cache_access_times.clear()
                return func(*args, **kwargs)
            except Exception as e:
                # Any other error - don't cache, just execute
                logger.warning(f"Cache error for {func.__name__}: {e}")
                return func(*args, **kwargs)
        return wrapper
    return decorator

# Cache clearing interval - prevent spam
CACHE_CLEAR_INTERVAL = 300  # 5 minutes minimum between full cache clears
_last_cache_clear_time = None

def clear_cache(pattern=None):
    """Clear cache entries matching pattern"""
    global _last_cache_clear_time
    try:
        from datetime import datetime
        current_time = datetime.utcnow()

        # If clearing entire cache, check interval to prevent spam
        if pattern is None:
            if _last_cache_clear_time:
                time_since_last_clear = (current_time - _last_cache_clear_time).total_seconds()
                if time_since_last_clear < CACHE_CLEAR_INTERVAL:
                    # Too soon - skip clearing to prevent spam
                    logger.debug(f"Cache clear skipped - only {time_since_last_clear:.1f}s since last clear (minimum {CACHE_CLEAR_INTERVAL}s)")
                    return

            _cache.clear()
            _cache_timestamps.clear()
            _cache_access_times.clear()
            _last_cache_clear_time = current_time
            logger.info("Cache cleared completely")
        else:
            keys_to_remove = [k for k in list(_cache.keys()) if pattern in k]
            for key in keys_to_remove:
                _cache.pop(key, None)
                _cache_timestamps.pop(key, None)
                _cache_access_times.pop(key, None)
            if keys_to_remove:
                logger.info(f"Cleared {len(keys_to_remove)} cache entries matching '{pattern}'")
            # Don't log if nothing to clear (prevent spam)
    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        # Force clear on error
        _cache.clear()
        _cache_timestamps.clear()
        _cache_access_times.clear()
        _last_cache_clear_time = datetime.utcnow() if 'datetime' in dir() else None

def get_cache_stats():
    """Get cache statistics"""
    try:
        return {
            'entries': len(_cache),
            'max_entries': MAX_CACHE_ENTRIES,
            'size_mb': round(_get_cache_size_mb(), 2),
            'max_size_mb': MAX_CACHE_SIZE_MB,
            'keys': list(_cache.keys())[:10]  # First 10 keys
        }
    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        return {
            'entries': 0,
            'max_entries': MAX_CACHE_ENTRIES,
            'size_mb': 0,
            'max_size_mb': MAX_CACHE_SIZE_MB,
            'keys': []
        }

# Response compression
import gzip

def compress_response(response):
    """Compress response if client supports it"""
    try:
        # Get request from response context
        from flask import request

        # Only compress if response is large enough and client accepts gzip
        accept_encoding = request.headers.get('Accept-Encoding', '') if hasattr(request, 'headers') else ''

        if 'gzip' not in accept_encoding:
            return response

        # Only compress text-based responses
        content_type = response.content_type or ''
        if not any(x in content_type for x in ['text', 'json', 'html', 'javascript', 'css', 'xml']):
            return response

        # Only compress if response is > 1KB
        data = response.get_data()
        if len(data) < 1024:
            return response

        # Compress
        compressed = gzip.compress(data, compresslevel=6)  # Level 6 = good balance
        response.set_data(compressed)
        response.headers['Content-Encoding'] = 'gzip'
        response.headers['Content-Length'] = len(compressed)
        response.headers['Vary'] = 'Accept-Encoding'
        logger.debug(f"Compressed response: {len(data)} -> {len(compressed)} bytes ({100*(1-len(compressed)/len(data)):.1f}% reduction)")
    except Exception as e:
        # If compression fails, return original response
        logger.debug(f"Compression skipped: {e}")

    return response

# Database query optimization
def optimize_query(query):
    """Optimize database query"""
    # SQLAlchemy already optimizes, but we can add hints
    return query

# Connection pooling (handled by SQLAlchemy, but we can verify settings)
def get_db_pool_settings():
    """Get optimal database pool settings"""
    return {
        'pool_size': 5,
        'max_overflow': 10,
        'pool_pre_ping': True,  # Verify connections before using
        'pool_recycle': 3600,  # Recycle connections after 1 hour
    }
